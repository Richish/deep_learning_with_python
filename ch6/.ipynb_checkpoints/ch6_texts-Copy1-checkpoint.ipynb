{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:44:14.204152Z",
     "start_time": "2020-05-11T16:44:14.200970Z"
    }
   },
   "source": [
    "# 2 types/techniques of converting tokens to tensors:\n",
    "1. One hot encoding.\n",
    "2. Word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T12:34:39.675206Z",
     "start_time": "2020-05-12T12:34:39.673277Z"
    }
   },
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:35.819984Z",
     "start_time": "2020-05-14T09:03:35.542677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 11)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a vector of n-word vector space, each word maps to a vector with one value corresponding to that word as one and others as 0.\n",
    "import numpy as np\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "token_index = {} # Tokenizes input via split method\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word]=len(token_index)+1\n",
    "max_len=10;\n",
    "results=np.zeros(shape=(len(samples), max_len, len(token_index)+1))\n",
    "# results is matrix representation of samples.\n",
    "# so first dimension is no. of sample, 2nd dimension is max length of each sample that will b econsidered(max no. of words ina sample)\n",
    "# and 3rd dimension is vector size of vector representation of each word.\n",
    "\n",
    "for i,sample in enumerate(samples):\n",
    "    for j, word in enumerate(sample.split()[:max_len]):\n",
    "        results[i,j,token_index[word]]=1\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### character level hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:35.828903Z",
     "start_time": "2020-05-14T09:03:35.821575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each character represented by a vector\n",
    "import string \n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable # all printable ascci characters, has nothing to do with samples\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "max_len=100\n",
    "\n",
    "results=np.zeros(shape=(len(samples), max_len, len(token_index)+1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index=token_index.get(character)\n",
    "        results[i, j, index] = 1\n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T13:06:41.659435Z",
     "start_time": "2020-05-12T13:06:41.657572Z"
    }
   },
   "source": [
    "### using keras for word level one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:38.487034Z",
     "start_time": "2020-05-14T09:03:35.831224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer=Tokenizer(num_words=1000) # create a tokenizer with 1000 most frequently used words only.\n",
    "\n",
    "tokenizer.fit_on_texts(samples) # fit the tokenizer of text samples (this part is where it finds things like 1000 most common words)\n",
    "sequences=tokenizer.texts_to_sequences(samples) # convert list of texts to list of integer indeces, where each index corresponds to a word\n",
    "# sequences will be [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
    "\n",
    "one_hot_results=tokenizer.texts_to_matrix(samples, mode='binary') # directly converts list of texts to one-hot encoded matrix\n",
    "#one_hot_results.shape = (2,1000)\n",
    "# each sentence is a vector, with 1 at locations corresponding to words that exist in sentence, 0 at other locations.\n",
    "# this is different form one_hot_encoding in 1.1.1, which contained info on placement of each word in the sentence as well.\n",
    "one_hot_results\n",
    "# modes can be binary, tfidf, count, freq -> last 2 will give count and frequency of each word in sentence.\n",
    "# frequency of a word is= count of word in sentence/total words in sentence\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index # recover the word index that was copied\n",
    "print('Found %s unique tokens.' % len(word_index), word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:38.491835Z",
     "start_time": "2020-05-14T09:03:38.488961Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:38.500238Z",
     "start_time": "2020-05-14T09:03:38.493518Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'mat': 5,\n",
       " 'dog': 6,\n",
       " 'ate': 7,\n",
       " 'my': 8,\n",
       " 'homework': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer=Tokenizer(num_words=1000)\n",
    "\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# sentence as sequence on numbers\n",
    "sequences=tokenizer.texts_to_sequences(samples)\n",
    "print(sequences)\n",
    "\n",
    "# directly one hot encode the sentences(gives 2*1000 ndarray)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "one_hot_results[1]\n",
    "\n",
    "#getting the word_index\n",
    "word_index=tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot hashing trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant of one-hot encoding is the so-called one-hot hashing trick, which you can use\n",
    "when the number of unique tokens in your vocabulary is too large to handle explicitly.\n",
    "Instead of explicitly assigning an index to each word and keeping a reference of these\n",
    "indices in a dictionary, you can hash words into vectors of fixed size. This is typically\n",
    "done with a very lightweight hashing function. The main advantage of this method is\n",
    "that it does away with maintaining an explicit word index, which saves memory and\n",
    "allows online encoding of the data (you can generate token vectors right away, before\n",
    "you’ve seen all of the available data). The one drawback of this approach is that it’s\n",
    "susceptible to hash collisions: two different words may end up with the same hash, and\n",
    "subsequently any machine-learning model looking at these hashes won’t be able to tell\n",
    "the difference between these words. The likelihood of hash collisions decreases when\n",
    "the dimensionality of the hashing space is much larger than the total number of\n",
    "unique tokens being hashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:38.508899Z",
     "start_time": "2020-05-14T09:03:38.501985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "dimensionality=1000\n",
    "max_length=10\n",
    "results=np.zeros((len(samples), max_length, dimensionality ))\n",
    "for i,sentence in enumerate(samples):\n",
    "    for j, word in enumerate(sentence[:max_length]):\n",
    "        word_hash=abs(hash(word))%dimensionality\n",
    "        results[i, j, word_hash] = 1.\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular and powerful way to associate a vector with a word is the use of dense\n",
    "word vectors, also called word embeddings. Whereas the vectors obtained through one-hot\n",
    "encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same\n",
    "dimensionality as the number of words in the vocabulary), word embeddings are lowdimensional\n",
    "floating-point vectors (that is, dense vectors, as opposed to sparse vectors);\n",
    "see figure 6.2. Unlike the word vectors obtained via one-hot encoding, word\n",
    "embeddings are learned from data. It’s common to see word embeddings that are\n",
    "256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large\n",
    "vocabularies. On the other hand, one-hot encoding words generally leads to vectors\n",
    "that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in\n",
    "this case). So, word embeddings pack more information into far fewer dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:16:00.121667Z",
     "start_time": "2020-04-27T02:16:00.118353Z"
    }
   },
   "source": [
    "### Learn word embeddings jointly with the main task\n",
    "Learn word embeddings jointly with the main task you care about (such as document\n",
    "classification or sentiment prediction). In this setup, you start with random\n",
    "word vectors and then learn word vectors in the same way you learn the\n",
    "weights of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating an Embedding layer\n",
    "The Embedding layer is best understood as a dictionary that maps integer indices\n",
    "(which stand for specific words) to dense vectors. It takes integers as input, it looks up\n",
    "these integers in an internal dictionary, and it returns the associated vectors. It’s effectively\n",
    "a dictionary lookup (see figure 6.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:38.516262Z",
     "start_time": "2020-05-14T09:03:38.510239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x642946198>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\"\"\"\n",
    "The Embedding layer takes at least two\n",
    "arguments: the number of possible tokens\n",
    "(here, 1,000: 1 + maximum word index)\n",
    "and the dimensionality of the embeddings\n",
    "(here, 64).\n",
    "\"\"\"\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "embedding_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer takes as input a 2D tensor of integers, of shape (samples,\n",
    "sequence_length), where each entry is a sequence of integers. It can embed\n",
    "sequences of variable lengths: for instance, you could feed into the Embedding layer in\n",
    "the previous example batches with shapes (32, 10) (batch of 32 sequences of length\n",
    "10) or (64, 15) (batch of 64 sequences of length 15). All sequences in a batch must\n",
    "have the same length, though (because you need to pack them into a single tensor),\n",
    "so sequences that are shorter than others should be padded with zeros, and sequences\n",
    "that are longer should be truncated.\n",
    "This layer returns a 3D floating-point tensor of shape (samples, sequence_\n",
    "length, embedding_dimensionality). Such a 3D tensor can then be processed by\n",
    "an RNN layer or a 1D convolution layer.\n",
    "\n",
    "When you instantiate an Embedding layer, its weights (its internal dictionary of\n",
    "token vectors) are initially random, just as with any other layer. During training, these\n",
    "word vectors are gradually adjusted via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:36:07.926126Z",
     "start_time": "2020-04-27T02:36:07.924061Z"
    }
   },
   "source": [
    "#### Loading the IMDB data for use with an Embedding layer\n",
    "restrict the\n",
    "movie reviews to the top 10,000 most common words (as you did the first time you\n",
    "worked with this dataset) and cut off the reviews after only 20 words. The network will\n",
    "learn 8-dimensional embeddings for each of the 10,000 words turn the input integer\n",
    "sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the\n",
    "tensor to 2D, and train a single Dense layer on top for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:42.923729Z",
     "start_time": "2020-05-14T09:03:38.519675Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features=10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# here each sample in x_train is list of integers(with length equal to no. of words in it.(only words in top 10k are included though))\n",
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:42.931130Z",
     "start_time": "2020-05-14T09:03:42.926674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 2, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 2, 4, 3586, 2]),\n",
       "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:43.201686Z",
     "start_time": "2020-05-14T09:03:42.932388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  65,   16,   38, ...,   19,  178,   32],\n",
       "       [  23,    4, 1690, ...,   16,  145,   95],\n",
       "       [1352,   13,  191, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [  11, 1818, 7561, ...,    4, 3586,    2],\n",
       "       [  92,  401,  728, ...,   12,    9,   23],\n",
       "       [ 764,   40,    4, ...,  204,  131,    9]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=20\n",
    "x_train=preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test=preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# making each review of length 20, by picking first 20 words only and if less that 20 words, adding padding chars.\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using an Embedding layer and classifier on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:03:43.269018Z",
     "start_time": "2020-05-14T09:03:43.203005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(10000, output_dim=8, input_length=max_len)) # turns 2d tensor into a 3d tensor of shape(samples, maxlen, 8). 8 is dimensionality\n",
    "# specified 10k so that it is convenient to flatten the o/p to 2d matrx later\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:04:07.538464Z",
     "start_time": "2020-05-14T09:03:43.270290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unravel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.6694 - acc: 0.6173 - val_loss: 0.6194 - val_acc: 0.7030\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 109us/step - loss: 0.5416 - acc: 0.7509 - val_loss: 0.5270 - val_acc: 0.7294\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 0.4600 - acc: 0.7901 - val_loss: 0.5012 - val_acc: 0.7492\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 0.4190 - acc: 0.8118 - val_loss: 0.4945 - val_acc: 0.7522\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 0.3907 - acc: 0.8260 - val_loss: 0.4960 - val_acc: 0.7532\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 111us/step - loss: 0.3677 - acc: 0.8399 - val_loss: 0.5002 - val_acc: 0.7578\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 108us/step - loss: 0.3483 - acc: 0.8498 - val_loss: 0.5051 - val_acc: 0.7544\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3302 - acc: 0.8600 - val_loss: 0.5128 - val_acc: 0.7560\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3133 - acc: 0.8687 - val_loss: 0.5195 - val_acc: 0.7538\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.2974 - acc: 0.8771 - val_loss: 0.5298 - val_acc: 0.7504\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:04:08.011116Z",
     "start_time": "2020-05-14T09:04:07.539951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 19us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5215603371810913, 0.7535600066184998]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrained word embeddings.\n",
    "Load into your model word embeddings that were precomputed using a different\n",
    "machine-learning task than the one you’re trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll use a model similar to the one we just went over: embedding sentences in\n",
    "sequences of vectors, flattening them, and training a Dense layer on top. But you’ll do\n",
    "so using pretrained word embeddings; and instead of using the pretokenized IMDB\n",
    "data packaged in Keras, you’ll start from scratch by downloading the original text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the labels of the raw IMDB data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:12:39.717727Z",
     "start_time": "2020-05-14T09:12:36.440411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir=\"data/aclImdb\"\n",
    "train_dir=os.path.join(imdb_dir, \"train\")\n",
    "test_dir=os.path.join(imdb_dir, \"test\")\n",
    "\n",
    "labels=[]\n",
    "texts=[]\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name=os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname.endswith(\".txt\"):\n",
    "            with open(os.path.join(dir_name, fname), 'r') as fp:\n",
    "                texts.append(fp.read())\n",
    "            if label_type=='neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "#labels, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the text of the raw IMDB data\n",
    "\n",
    "Let’s vectorize the text and prepare a training and validation split, using the concepts\n",
    "introduced earlier in this section. Because pretrained word embeddings are meant to\n",
    "be particularly useful on problems where little training data is available (otherwise,\n",
    "task-specific embeddings are likely to outperform them), we’ll add the following twist:\n",
    "restricting the training data to the first 200 samples. So you’ll learn to classify movie\n",
    "reviews after looking at just 200 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:12:55.586440Z",
     "start_time": "2020-05-14T09:12:47.685612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_len=100 # cuts off reviews after 100 words\n",
    "training_samples=200\n",
    "validation_samples=10000\n",
    "max_words=10000\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:13:12.408103Z",
     "start_time": "2020-05-14T09:13:12.146409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, max_len)\n",
    "labels=np.asarray(labels)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:13:21.963538Z",
     "start_time": "2020-05-14T09:13:21.951580Z"
    }
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and parse glove word encodeings\n",
    "learn embeddings from glove database\n",
    "it is a database of 2014 wikipedia articles of more that few 100 thousand articles, 400k word vectors and 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:13:50.575055Z",
     "start_time": "2020-05-14T09:13:37.268438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir=\"data/glove.6B\"\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the GloVe word-embeddings matrix\n",
    "build an embedding matrix that you can load into an Embedding layer. It\n",
    "must be a matrix of shape (max_words, embedding_dim), where each entry i contains\n",
    "the embedding_dim-dimensional vector for the word of index i in the reference word\n",
    "index (built during tokenization). Note that index 0 isn’t supposed to stand for any\n",
    "word or token—it’s a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:04:33.253173Z",
     "start_time": "2020-05-14T09:04:33.251056Z"
    }
   },
   "outputs": [],
   "source": [
    "#embeddings_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:14:07.920335Z",
     "start_time": "2020-05-14T09:14:07.881961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
       "         0.82779998,  0.27061999],\n",
       "       [-0.071953  ,  0.23127   ,  0.023731  , ..., -0.71894997,\n",
       "         0.86894   ,  0.19539   ],\n",
       "       ...,\n",
       "       [-0.44036001,  0.31821999,  0.10778   , ..., -1.29849994,\n",
       "         0.11824   ,  0.64845002],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.54539001, -0.31817999, -0.016281  , ..., -0.44865   ,\n",
       "         0.067047  ,  0.17975999]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=100\n",
    "embedding_matrix=np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i<max_words:\n",
    "        try:\n",
    "            embedding_vector=embeddings_index[word]\n",
    "        except:\n",
    "            #print(word)\n",
    "            continue\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:14:18.679483Z",
     "start_time": "2020-05-14T09:14:18.588570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using same model architecture s before: embedding->flatten->dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting weights of embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer has a single weight matrix: a 2D float matrix where each entry i is\n",
    "the word vector meant to be associated with index i. Simple enough. Load the GloVe\n",
    "matrix you prepared into the Embedding layer, the first layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:14:28.850020Z",
     "start_time": "2020-05-14T09:14:28.839237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 320,065\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable=False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compile and train and see the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:14:55.480122Z",
     "start_time": "2020-05-14T09:14:48.108387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 1.6757 - acc: 0.5200 - val_loss: 0.7047 - val_acc: 0.4994\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6452 - acc: 0.5950 - val_loss: 0.7011 - val_acc: 0.5043\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6711 - acc: 0.6650 - val_loss: 0.7352 - val_acc: 0.5065\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3864 - acc: 0.8850 - val_loss: 1.1907 - val_acc: 0.4990\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2928 - acc: 0.8850 - val_loss: 0.7160 - val_acc: 0.5670\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3721 - acc: 0.8550 - val_loss: 0.7916 - val_acc: 0.5470\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1035 - acc: 0.9950 - val_loss: 0.7816 - val_acc: 0.5681\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1226 - acc: 0.9750 - val_loss: 2.8365 - val_acc: 0.4987\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2336 - acc: 0.9150 - val_loss: 0.7194 - val_acc: 0.5973\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0317 - acc: 1.0000 - val_loss: 0.7711 - val_acc: 0.5832\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32)\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:52.383621Z",
     "start_time": "2020-05-14T08:51:52.251165Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:04:39.148406Z",
     "start_time": "2020-05-14T09:04:39.144429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.figure()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is causing kernel to die\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:04:39.153392Z",
     "start_time": "2020-05-14T09:04:39.150582Z"
    }
   },
   "outputs": [],
   "source": [
    "#plt.rcdefaults()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the same model without pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give much lower validation accuracy if number of sample is still lower, will increase if train samples increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:05:25.155120Z",
     "start_time": "2020-05-14T09:05:17.877816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unravel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.6949 - acc: 0.5100 - val_loss: 0.6939 - val_acc: 0.5059\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4833 - acc: 1.0000 - val_loss: 0.7010 - val_acc: 0.5098\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2668 - acc: 1.0000 - val_loss: 0.7015 - val_acc: 0.5153\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1126 - acc: 1.0000 - val_loss: 0.7131 - val_acc: 0.5185\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0523 - acc: 1.0000 - val_loss: 0.7232 - val_acc: 0.5178\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0272 - acc: 1.0000 - val_loss: 0.7228 - val_acc: 0.5254\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.7304 - val_acc: 0.5263\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.7414 - val_acc: 0.5270\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.7466 - val_acc: 0.5251\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7538 - val_acc: 0.5255\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,epochs=10,batch_size=32,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:20:55.997330Z",
     "start_time": "2020-05-14T09:20:51.362754Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(os.path.join(dir_name, fname), 'r') as f:\n",
    "                texts.append(f.read())\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=max_len)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:21:34.511567Z",
     "start_time": "2020-05-14T09:21:33.529660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 38us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7854385869979859, 0.5796800255775452]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo code for RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let’s implement the forward pass of a\n",
    "toy RNN in Numpy. This RNN takes as input a sequence of vectors, which you’ll encode\n",
    "as a 2D tensor of size (timesteps, input_features). It loops over timesteps, and at\n",
    "each timestep, it considers its current state at t and the input at t (of shape (input_\n",
    "features,), and combines them to obtain the output at t. You’ll then set the state for\n",
    "the next step to be this previous output. For the first timestep, the previous output\n",
    "isn’t defined; hence, there is no current state. So, you’ll initialize the state as an allzero\n",
    "vector called the initial state of the network.\n",
    "In pseudocode, this is the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t=0\n",
    "for input_t in input_sequence:\n",
    "    output_t=f(input_t, state_t)\n",
    "    state_t=output_t # output_t becomes the state_t for next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even flesh out the function f: the transformation of the input and state into an\n",
    "output will be parameterized by two matrices, W and U, and a bias vector. It’s similar to\n",
    "the transformation operated by a densely connected layer in a feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t=0\n",
    "for input in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, input_t) + bias)\n",
    "    state_t=output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN implementation using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T10:16:02.258027Z",
     "start_time": "2020-05-14T10:16:02.244980Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "timesteps=100\n",
    "input_features=32\n",
    "output_features=64\n",
    "\n",
    "input_sequence=np.random.random((timesteps, input_features))\n",
    "# hence input_t will be a vector of shape(input_features,)\n",
    "\n",
    "# initializing initial state, W, U and bias\n",
    "state_t=np.zeros((output_features,))\n",
    "W=np.random.random((output_features, input_features))\n",
    "U=np.random.random((output_features, output_features))\n",
    "bias=np.random.random((output_features,))\n",
    "\n",
    "# Doing 1 forward pass of whole sequence\n",
    "successive_outputs=[]\n",
    "for input_t in input_sequence:\n",
    "    output_t=np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + bias)\n",
    "    # storing outpu_t at each step in a list\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t=output_t # using output of current iteration as state of next iteration\n",
    "# finally store the output sequence in a np array\n",
    "final_output_sequence=np.concatenate(successive_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T10:18:13.778977Z",
     "start_time": "2020-05-14T10:18:13.775484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output_sequence.shape #2d tensor of shape (timesteps, output_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, an RNN is a for loop that reuses quantities computed\n",
    "during the previous iteration of the loop, nothing more. Of course, there are many\n",
    "different RNNs fitting this definition that you could build—this example is one of the\n",
    "simplest RNN formulations. RNNs are characterized by their step function, such as the\n",
    "following function in this case:\n",
    "output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent layers in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process you just naively implemented in Numpy corresponds to an actual Keras\n",
    "layer—the SimpleRNN layer:\n",
    "from keras.layers import SimpleRNN\n",
    "There is one minor difference: SimpleRNN processes batches of sequences, like all other\n",
    "Keras layers, not a single sequence as in the Numpy example. This means it takes inputs\n",
    "of shape (batch_size, timesteps, input_features), rather than (timesteps,\n",
    "input_features).\n",
    "Like all recurrent layers in Keras, SimpleRNN can be run in two different modes: it\n",
    "can return either the full sequences of successive outputs for each timestep (a 3D tensor\n",
    "of shape (batch_size, timesteps, output_features)) or only the last output for\n",
    "each input sequence (a 2D tensor of shape (batch_size, output_features)). These\n",
    "two modes are controlled by the return_sequences constructor argument. Let’s look\n",
    "at an example that uses SimpleRNN and returns only the output at the last timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:13:41.259302Z",
     "start_time": "2020-05-14T16:13:41.175015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "model=Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()\n",
    "# this only outputs the output for last sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:15:00.511270Z",
     "start_time": "2020-05-14T16:15:00.426853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, None, 32)          2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# output from all batches:\n",
    "model=Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking rnn layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s sometimes useful to stack several recurrent layers one after the other in order to\n",
    "increase the representational power of a network. In such a setup, you have to get all\n",
    "of the intermediate layers to return full sequence of outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:19:11.543763Z",
     "start_time": "2020-05-14T16:19:11.306669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 328,320\n",
      "Trainable params: 328,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(10_000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=False)) # last layer may or may not return the output of full sequence\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USing above(stacked-simple) RNN for movie review classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T21:55:00.911688Z",
     "start_time": "2020-05-14T21:54:55.793312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "(25000,) (25000,)\n",
      "padding sequences..\n",
      "(25000, 500) (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features=10_000\n",
    "maxlen=500\n",
    "batch_size=32\n",
    "\n",
    "print('loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(input_train.shape, input_test.shape)\n",
    "print(\"padding sequences..\")\n",
    "input_train=sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test=sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print(input_train.shape, input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training with single simple rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-14T21:24:05.967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unravel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "embedding_dimensions=32\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features, embedding_dimensions))\n",
    "model.add(SimpleRNN(embedding_dimensions))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(input_train, y_train, batch_size=batch_size, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will give memory error in local laptop, train on colab gives- accuracy of 85%\n",
    "Which is low compared to simple dense networs. 2 reasons:\n",
    "    1. it looked at only 500 words not all.\n",
    "    2. simple rnn is not good enough for processing long sequences. Will have to use other RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T21:33:10.327597Z",
     "start_time": "2020-05-14T21:33:10.325576Z"
    }
   },
   "source": [
    "## LSTM and GRU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using lstm in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-14T21:55:05.102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unravel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,epochs=10,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch64_1dconv_for_sequence_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM238lYmE3BLrdUOxQB8YUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/deep_learning_with_python/blob/master/ch64_1dconv_for_sequence_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz7yDjTcDNP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmSbF0NoDZBc",
        "colab_type": "text"
      },
      "source": [
        "# imdb classification using 1d conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNmkiuiDpsY",
        "colab_type": "text"
      },
      "source": [
        "## Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPr836YIDYWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3d18c7d4-7f55-42ad-fc95-7bd1d6c8c7ec"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_len=500\n",
        "max_features=10_000\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, maxlen=max_len)\n",
        "\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train=sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test=sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "25000 train sequences\n",
            "20947 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (20947, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8RrNGS9RXzu",
        "colab_type": "text"
      },
      "source": [
        "## build, train, evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgjxkWcJFxX1",
        "colab_type": "text"
      },
      "source": [
        "1D convnets are structured in the same way as their 2D counterparts, which you used\n",
        "in chapter 5: they consist of a stack of Conv1D and MaxPooling1D layers, ending in\n",
        "either a global pooling layer or a Flatten layer, that turn the 3D outputs into 2D outputs,\n",
        "allowing you to add one or more Dense layers to the model for classification or\n",
        "regression.\n",
        "One difference, though, is the fact that you can afford to use larger convolution\n",
        "windows with 1D convnets. With a 2D convolution layer, a 3 × 3 convolution window\n",
        "contains 3 × 3 = 9 feature vectors; but with a 1D convolution layer, a convolution window\n",
        "of size 3 contains only 3 feature vectors. You can thus easily afford 1D convolution\n",
        "windows of size 7 or 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0MaQ4MXFy6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a87b06e1-3b09-4c36-c91a-648f24b75a32"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(Conv1D(32, 7, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(32, 7, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 494, 32)           28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 98, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_11 (Conv1D)           (None, 92, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_4 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,315,937\n",
            "Trainable params: 1,315,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzi581S3Fy9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "9ebfbc2e-8c0a-4567-df7c-5182826ba7c7"
      },
      "source": [
        "model.compile(optimizer=RMSprop(lr=1e-4),loss='binary_crossentropy',metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,epochs=10,batch_size=128,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 70s 4ms/step - loss: 0.8688 - acc: 0.5159 - val_loss: 0.6929 - val_acc: 0.4932\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 70s 3ms/step - loss: 0.6758 - acc: 0.6179 - val_loss: 0.6744 - val_acc: 0.6374\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 70s 3ms/step - loss: 0.6442 - acc: 0.7255 - val_loss: 0.6382 - val_acc: 0.7166\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 70s 3ms/step - loss: 0.5744 - acc: 0.7843 - val_loss: 0.5421 - val_acc: 0.7734\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 70s 3ms/step - loss: 0.4456 - acc: 0.8342 - val_loss: 0.4255 - val_acc: 0.8196\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.3645 - acc: 0.8591 - val_loss: 0.3998 - val_acc: 0.8294\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.3160 - acc: 0.8606 - val_loss: 0.4106 - val_acc: 0.8166\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.2877 - acc: 0.8547 - val_loss: 0.4176 - val_acc: 0.8074\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.2634 - acc: 0.8385 - val_loss: 0.4527 - val_acc: 0.7772\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.2388 - acc: 0.8185 - val_loss: 0.4535 - val_acc: 0.7762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCm36dq-RHXv",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluating a simple 1D convnet on the Jena data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLUGUefeRJDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "57e9bb98-6548-4b60-d400-1f55f60ab419"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "13574144/13568290 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x8DF8-2R7eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ca84080-50a7-4215-8230-58cdd8be0ebf"
      },
      "source": [
        "df=pd.read_csv(csv_path)\n",
        "df=df.drop(columns='Date Time')\n",
        "df.keys()\n",
        "mean=df.loc[:200_000].mean()\n",
        "std=df.loc[:200_000].std()\n",
        "df-=mean\n",
        "df/=std\n",
        "df.head()\n",
        "\n",
        "float_data=df.to_numpy()\n",
        "float_data.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(420551, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR6ygWuPTDl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
        "    \"\"\"\n",
        "    data: np.array > of full data set.\n",
        "    lookback: no. of datapoints to look back in each sample.\n",
        "    delay: no. of datapoints after current sample for which prediction is to be made, or which corresponds to y\n",
        "    min_index: consider datapoints starting from this index only\n",
        "    max_index: Take datapoint upto max this index only\n",
        "    shuffle: shuffle the data before taking a batch out of data and yielding that\n",
        "    batch_size: no. of samples to be yielded in a single batch\n",
        "    step: not all data points will be considered, only datapoints at frequency- 'step' will be considered. \n",
        "            since data points aare at 10 min interval so step=6 implies only hourly data will be considered.\n",
        "    return: a tuple of (samples, data_samples_at_delay). Samples is a 2d np array of shape: (batch_size, lookback, data.shape[-1]). Basically a a batch of samples, \n",
        "            where each sample contains a np array of shape(lookback, data.shape[-1])\n",
        "            data_samples_at_delay is a 2d np array of shape: (batch_size, 1)- 1 since we only need temperature as output\n",
        "    \"\"\"\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback # which rows that will contain the end of sample data, delay will be relative to this row.\n",
        "    while True:\n",
        "        if shuffle:\n",
        "            # rows will be equal to batch_size\n",
        "            rows=np.random.randint(i, max_index, size=batch_size) # basically row indexes.\n",
        "        else: # not shuffled\n",
        "            if i+batch_size>max_index: # if reached end, need to rotate and go to beginning to get new batch\n",
        "                i=min_index+lookback\n",
        "            rows=np.arange(i, min(i+batch_size,max_index))\n",
        "            i+=len(rows) # move pointer to where end of next batch would be\n",
        "        # initializing samples and targets to 0s.\n",
        "        samples=np.zeros((len(rows), lookback//step, data.shape[-1]))\n",
        "        targets=np.zeros((len(rows),))\n",
        "        # setting the value of samples and targets for each sample/target in a batch of batch_size\n",
        "        for j, row in enumerate(rows):\n",
        "            indeces=np.arange(row-lookback, row, step)\n",
        "            samples[j]=data[indeces]\n",
        "            targets[j]=data[row+delay][1] # we only need the temperature as output/label\n",
        "        yield samples, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fefPHVkbW1Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lookback = 24*5*6 # 5 days data\n",
        "step = 6\n",
        "delay = 24*6 # 1 day's delay/prediction\n",
        "batch_size = 128 # training batch size\n",
        "\n",
        "train_gen = generator(float_data,\n",
        "                lookback=lookback,\n",
        "                delay=delay,\n",
        "                min_index=0,\n",
        "                max_index=200_000,\n",
        "                shuffle=True,\n",
        "                step=step,\n",
        "                batch_size=batch_size)\n",
        "\n",
        "val_gen = generator(float_data,\n",
        "                lookback=lookback,\n",
        "                delay=delay,\n",
        "                min_index=200_001,\n",
        "                max_index=300_000,\n",
        "                shuffle=True,\n",
        "                step=step,\n",
        "                batch_size=batch_size)\n",
        "\n",
        "test_gen = generator(float_data,\n",
        "                lookback=lookback,\n",
        "                delay=delay,\n",
        "                min_index=300_001,\n",
        "                max_index=None,\n",
        "                shuffle=True,\n",
        "                step=step,\n",
        "                batch_size=batch_size)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback) # how many val steps to look at whole data\n",
        "test_steps = (len(float_data) - 300001 - lookback) # how many test steps to look at whole data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2InH7jgW2A3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "cafa7b78-215c-42a3-e482-df4913630150"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation='relu',\n",
        "input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, None, 32)          2272      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, None, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 32)          5152      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, None, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, None, 32)          5152      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 12,609\n",
            "Trainable params: 12,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSU-sGTxW2G5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "5e38db3a-8eb5-43b0-82b5-c9a444d677a9"
      },
      "source": [
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "steps_per_epoch=500,\n",
        "epochs=20,\n",
        "validation_data=val_gen,\n",
        "validation_steps=val_steps/100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.2187 - val_loss: 0.3595\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.2149 - val_loss: 0.3834\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.2110 - val_loss: 0.4002\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.2088 - val_loss: 0.3910\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.2058 - val_loss: 0.3824\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.2025 - val_loss: 0.3858\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.2007 - val_loss: 0.4427\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1991 - val_loss: 0.3648\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1985 - val_loss: 0.3951\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1940 - val_loss: 0.4167\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1936 - val_loss: 0.4675\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1907 - val_loss: 0.4501\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1901 - val_loss: 0.4448\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1871 - val_loss: 0.3715\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1867 - val_loss: 0.4231\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1847 - val_loss: 0.4396\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1831 - val_loss: 0.4703\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1812 - val_loss: 0.4314\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1811 - val_loss: 0.4359\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 0.1798 - val_loss: 0.4198\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}